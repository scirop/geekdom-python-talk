{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Article is posted [here](https://medium.com/@swarupsahoo/preparing-text-for-machine-learning-60396e4e393)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import string\n",
    "import nltk\n",
    "import sys\n",
    "import numpy as np\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/dotnet/machinelearning\\\n",
    "/master/test/data/wikipedia-detox-250-line-data.tsv',\n",
    "            sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>==RUDE== Dude, you are rude upload that carl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>== OK! ==  IM GOING TO VANDALIZE WILD ONES W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Stop trolling, zapatancas, calling me a lia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>==You're cool==  You seem like a really cool...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>::::: Why are you threatening me? I'm not bei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>== hey waz up? ==  hey ummm... the fif four ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>::::::::::I'm not sure either. I think it has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>*::Your POV and propaganda pushing is dully n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>== File:Hildebrandt-Greg and Tim.jpg listed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>::::::::This is a gross exaggeration. Nobody...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>::No, I won't unrevert your edits!\" \"sounds mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>::::I heard Mark Kermode say today that Turbo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>I am a sock puppet? THAT is my ban reason? ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>You twit, read the article before you rever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>A tag has been placed on Jerome leung kam, re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>==READ THIS== This is Wikipedia. It is a pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>== Administrator Complaint Filed Against You...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>It is a shame what people are here, I am dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>:Hello Cielomobile. I have to say that I als...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>== my removal of your content on DNA melting...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentiment                                      SentimentText\n",
       "0           1    ==RUDE== Dude, you are rude upload that carl...\n",
       "1           1    == OK! ==  IM GOING TO VANDALIZE WILD ONES W...\n",
       "2           1     Stop trolling, zapatancas, calling me a lia...\n",
       "3           1    ==You're cool==  You seem like a really cool...\n",
       "4           1   ::::: Why are you threatening me? I'm not bei...\n",
       "5           1    == hey waz up? ==  hey ummm... the fif four ...\n",
       "6           0   ::::::::::I'm not sure either. I think it has...\n",
       "7           0   *::Your POV and propaganda pushing is dully n...\n",
       "8           0    == File:Hildebrandt-Greg and Tim.jpg listed ...\n",
       "9           0    ::::::::This is a gross exaggeration. Nobody...\n",
       "10          1  ::No, I won't unrevert your edits!\" \"sounds mo...\n",
       "11          1   ::::I heard Mark Kermode say today that Turbo...\n",
       "12          1     I am a sock puppet? THAT is my ban reason? ...\n",
       "13          1     You twit, read the article before you rever...\n",
       "14          0   A tag has been placed on Jerome leung kam, re...\n",
       "15          1    ==READ THIS== This is Wikipedia. It is a pla...\n",
       "16          0    == Administrator Complaint Filed Against You...\n",
       "17          1    It is a shame what people are here, I am dis...\n",
       "18          0    :Hello Cielomobile. I have to say that I als...\n",
       "19          0    == my removal of your content on DNA melting..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df['SentimentText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt', quiet=True, raise_on_error=True)\n",
    "stemmer = SnowballStemmer('english')\n",
    "p_map = dict((ord(char),None) for char in string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords', quiet=True, raise_on_error=True)\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "tokenized_stop_words = nltk.word_tokenize(' '.join(nltk.corpus.stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(item) for item in tokens if item not in tokenized_stop_words]\n",
    "def normalize(text):\n",
    "    return stem_tokens(nltk.word_tokenize(text.lower().translate(p_map)))\n",
    "\n",
    "tfidf = TfidfVectorizer(use_idf=True, \n",
    "                        ngram_range=(1,3), \n",
    "                        strip_accents='unicode',\n",
    "                        tokenizer=normalize, \n",
    "                        analyzer='word',\n",
    "                        max_features = 10000\n",
    "                       )\n",
    "  \n",
    "tfidf_score = tfidf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0',\n",
       " '0 googl',\n",
       " '0 googl hit',\n",
       " '0 hit',\n",
       " '0 hit holocaust',\n",
       " '0 imag',\n",
       " '0 imag copyright',\n",
       " '0 newslett',\n",
       " '0 newslett thank',\n",
       " '0 thanx',\n",
       " '0 thanx efe',\n",
       " '0 woah',\n",
       " '0 woah someon',\n",
       " '0518',\n",
       " '0518 utc',\n",
       " '0548',\n",
       " '0548 3',\n",
       " '0548 3 decemb',\n",
       " '10',\n",
       " '10 year',\n",
       " '10 year franc',\n",
       " '100',\n",
       " '100 im',\n",
       " '100 im go',\n",
       " '100 new',\n",
       " '100 new page',\n",
       " '100 page',\n",
       " '100 page address',\n",
       " '100 page still',\n",
       " '100 wp',\n",
       " '100 wp page',\n",
       " '11year',\n",
       " '11year old',\n",
       " '11year old mayb',\n",
       " '130',\n",
       " '130 year',\n",
       " '130 year old',\n",
       " '130000',\n",
       " '130000 year',\n",
       " '130000 year old',\n",
       " '13100',\n",
       " '13100 hit',\n",
       " '13100 hit big',\n",
       " '1423',\n",
       " '15',\n",
       " '15 year',\n",
       " '15 year old',\n",
       " '16',\n",
       " '16 august',\n",
       " '16 august 2008',\n",
       " '1682099734',\n",
       " '1682099734 basi',\n",
       " '1682099734 basi acus',\n",
       " '1717',\n",
       " '1717 utc',\n",
       " '17419166126',\n",
       " '17419166126 aka',\n",
       " '17419166126 aka 1741916992',\n",
       " '1741916992',\n",
       " '1741916992 appar',\n",
       " '1741916992 appar edit',\n",
       " '1987',\n",
       " '1987 sold',\n",
       " '1987 sold year',\n",
       " '1988',\n",
       " '1988 show',\n",
       " '1988 show case',\n",
       " '1989',\n",
       " '1989 jackson',\n",
       " '1989 jackson made',\n",
       " '1994',\n",
       " '1994 1996',\n",
       " '1994 1996 sinc',\n",
       " '1994 sale',\n",
       " '1994 sale publish',\n",
       " '1996',\n",
       " '1996 sinc',\n",
       " '1996 sinc discredit',\n",
       " '1996 usual',\n",
       " '1996 usual estim',\n",
       " '1998',\n",
       " '1998 due',\n",
       " '1998 due declin',\n",
       " '1the',\n",
       " '1the diot',\n",
       " '1the diot write',\n",
       " '2',\n",
       " '2 dont',\n",
       " '2 dont worri',\n",
       " '2 theyr',\n",
       " '2 theyr tri',\n",
       " '2 word',\n",
       " '2 word learn',\n",
       " '200',\n",
       " '200 thousand',\n",
       " '200 thousand year',\n",
       " '2002',\n",
       " '2002 reveal',\n",
       " '2002 reveal debt',\n",
       " '2003',\n",
       " '2003 confirm',\n",
       " '2003 confirm verg',\n",
       " '2005',\n",
       " '2005 0518',\n",
       " '2005 0518 utc',\n",
       " '2005 1717',\n",
       " '2005 1717 utc',\n",
       " '2005 2121',\n",
       " '2005 2121 utc',\n",
       " '2008',\n",
       " '2008 utc',\n",
       " '2008 utc im',\n",
       " '2009',\n",
       " '2009 pleas',\n",
       " '2009 pleas attack',\n",
       " '2013',\n",
       " '2013 want',\n",
       " '2013 want understand',\n",
       " '2013\\u200e',\n",
       " '2013\\u200e someon',\n",
       " '2013\\u200e someon list',\n",
       " '2014',\n",
       " '2014 utc',\n",
       " '2014 utc ok',\n",
       " '2064524242',\n",
       " '2064524242 talk',\n",
       " '20yearold',\n",
       " '20yearold still',\n",
       " '20yearold still admir',\n",
       " '2121',\n",
       " '2121 utc',\n",
       " '22',\n",
       " '22 year',\n",
       " '22 year most',\n",
       " '2238',\n",
       " '3',\n",
       " '3 decemb',\n",
       " '3 decemb 2013\\u200e',\n",
       " '3 juli',\n",
       " '3 juli 2005',\n",
       " '30',\n",
       " '30 june',\n",
       " '30 june 2005',\n",
       " '30 million',\n",
       " '30 million copi',\n",
       " '306000',\n",
       " '306000 googl',\n",
       " '306000 googl hit',\n",
       " '306000 hit',\n",
       " '306000 hit 486',\n",
       " '32',\n",
       " '32 base32',\n",
       " '32 base32 edit',\n",
       " '37th',\n",
       " '37th best',\n",
       " '37th best sell',\n",
       " '4',\n",
       " '4 100',\n",
       " '4 100 page',\n",
       " '4 join',\n",
       " '4 join euro',\n",
       " '4 main',\n",
       " '4 main point',\n",
       " '400',\n",
       " '400 million',\n",
       " '400 million invinc',\n",
       " '45',\n",
       " '45 ancestor',\n",
       " '45 ancestor worship',\n",
       " '45 nonbeliev',\n",
       " '45 nonbeliev bit',\n",
       " '486',\n",
       " '486 hit',\n",
       " '486 hit holocaust',\n",
       " '5',\n",
       " '5 greec',\n",
       " '5 greec fiscal',\n",
       " '5 juli',\n",
       " '5 juli 2005',\n",
       " '5 year',\n",
       " '50',\n",
       " '50 buck',\n",
       " '50 buck week',\n",
       " '5000',\n",
       " '5000 rhino',\n",
       " '5000 rhino meal',\n",
       " '6',\n",
       " '6 januari',\n",
       " '6 januari 2014',\n",
       " '6 pov',\n",
       " '6 pov lead',\n",
       " '7',\n",
       " '7 pillar',\n",
       " '80',\n",
       " '80 rubbish',\n",
       " '800',\n",
       " '800 byte',\n",
       " '800 byte info',\n",
       " '99',\n",
       " '99 troll',\n",
       " '99 troll therefor',\n",
       " '9925417230',\n",
       " '9925417230 remov',\n",
       " '9925417230 remov pertin',\n",
       " 'aback',\n",
       " 'aback want',\n",
       " 'aback want know',\n",
       " 'abf',\n",
       " 'abf treat',\n",
       " 'abf treat like',\n",
       " 'absolut',\n",
       " 'absolut ridicul',\n",
       " 'absolut ridicul long',\n",
       " 'absolut unequivoc',\n",
       " 'absolut unequivoc unquestion',\n",
       " 'abus',\n",
       " 'abus admin',\n",
       " 'abus admin get',\n",
       " 'abus jerk',\n",
       " 'abus jerk place',\n",
       " 'abus realli',\n",
       " 'abus realli came',\n",
       " 'abus tool',\n",
       " 'abus topic',\n",
       " 'abus topic like',\n",
       " 'accept',\n",
       " 'accept argument',\n",
       " 'accept argument alan',\n",
       " 'accept compromis',\n",
       " 'accept compromis view',\n",
       " 'accept jack',\n",
       " 'accept jack napier',\n",
       " 'accept languag',\n",
       " 'accept languag wikipedia',\n",
       " 'accept nextdoorneighbour',\n",
       " 'accept nextdoorneighbour mr',\n",
       " 'accept notabl',\n",
       " 'accept notabl think',\n",
       " 'accept orthodox',\n",
       " 'accept orthodox pov',\n",
       " 'accept relev',\n",
       " 'accept relev smcs',\n",
       " 'access',\n",
       " 'access 7',\n",
       " 'access 7 pillar',\n",
       " 'access better',\n",
       " 'access better sourc',\n",
       " 'access make',\n",
       " 'access make sens',\n",
       " 'accid',\n",
       " 'accid die',\n",
       " 'accid die suicid',\n",
       " 'accident',\n",
       " 'accident suicid',\n",
       " 'accident suicid made',\n",
       " 'accord',\n",
       " 'accord propos',\n",
       " 'accord propos delet',\n",
       " 'account',\n",
       " 'account avoid',\n",
       " 'account avoid irrelev',\n",
       " 'account thank',\n",
       " 'account thank ignor',\n",
       " 'accur',\n",
       " 'accur general',\n",
       " 'accur general strict',\n",
       " 'accur pepl',\n",
       " 'accur pepl may',\n",
       " 'accus',\n",
       " 'accus cringeworthi',\n",
       " 'accus cringeworthi act',\n",
       " 'accus current',\n",
       " 'accus current second',\n",
       " 'accus other',\n",
       " 'accus other get',\n",
       " 'accus peopl',\n",
       " 'accus peopl sockpuppetri',\n",
       " 'accus puppet',\n",
       " 'accus puppet nonsens',\n",
       " 'act',\n",
       " 'act annoy',\n",
       " 'act annoy favourit',\n",
       " 'act donkey',\n",
       " 'act donkey sprotect',\n",
       " 'act function',\n",
       " 'act function illiter',\n",
       " 'act good',\n",
       " 'act good faith',\n",
       " 'act hostil',\n",
       " 'act hostil get',\n",
       " 'act reckless',\n",
       " 'act reckless inconsist',\n",
       " 'action',\n",
       " 'action alreadi',\n",
       " 'action alreadi send',\n",
       " 'action mayb',\n",
       " 'action mayb your',\n",
       " 'action ps',\n",
       " 'action ps repli',\n",
       " 'action pure',\n",
       " 'action pure vandal',\n",
       " 'action threaten',\n",
       " 'action threaten fight',\n",
       " 'activ',\n",
       " 'activ career',\n",
       " 'activ career finish',\n",
       " 'activ wwe',\n",
       " 'activ wwe cant',\n",
       " 'actual',\n",
       " 'actual attack',\n",
       " 'actual attack instead',\n",
       " 'actual call',\n",
       " 'actual call war',\n",
       " 'actual cockroach',\n",
       " 'actual cockroach follow',\n",
       " 'actual didnt',\n",
       " 'actual didnt even',\n",
       " 'actual get',\n",
       " 'actual get 0',\n",
       " 'acus',\n",
       " 'acus user',\n",
       " 'acus user pleas',\n",
       " 'ad',\n",
       " 'ad accur',\n",
       " 'ad accur general',\n",
       " 'ad appropri',\n",
       " 'ad appropri tag',\n",
       " 'ad articl',\n",
       " 'ad articl kobe',\n",
       " 'ad fault',\n",
       " 'ad fault profession',\n",
       " 'ad infinitum',\n",
       " 'ad infinitum ad',\n",
       " 'ad nauseum',\n",
       " 'ad nauseum proof',\n",
       " 'ad nonsens',\n",
       " 'ad nonsens wikipedia',\n",
       " 'ad point',\n",
       " 'ad point includ',\n",
       " 'ad refernc',\n",
       " 'ad refernc link',\n",
       " 'ad tag',\n",
       " 'ad tag ad',\n",
       " 'add',\n",
       " 'add add',\n",
       " 'add add pleas',\n",
       " 'add copyright',\n",
       " 'add copyright tag',\n",
       " 'add effect',\n",
       " 'add effect threat',\n",
       " 'add fals',\n",
       " 'add fals tag',\n",
       " 'add inform',\n",
       " 'add inform articl',\n",
       " 'add list',\n",
       " 'add list fool',\n",
       " 'add note',\n",
       " 'add note figur',\n",
       " 'add paragraph',\n",
       " 'add paragraph detail',\n",
       " 'add pleas',\n",
       " 'add pleas show',\n",
       " 'add someon',\n",
       " 'add someon talk',\n",
       " 'add top',\n",
       " 'add top kobe',\n",
       " 'add top page',\n",
       " 'addchangedelet',\n",
       " 'addchangedelet propos',\n",
       " 'addchangedelet propos main',\n",
       " 'addit',\n",
       " 'addit concern',\n",
       " 'addit concern airlin',\n",
       " 'addit continu',\n",
       " 'addit continu point',\n",
       " 'addit list',\n",
       " 'addit list malcolm',\n",
       " 'addit taken',\n",
       " 'addit taken time',\n",
       " 'address',\n",
       " 'address attach',\n",
       " 'address attach pdf',\n",
       " 'address didnt',\n",
       " 'address didnt make',\n",
       " 'address id',\n",
       " 'address id like',\n",
       " 'address issu',\n",
       " 'address issu rais',\n",
       " 'address king',\n",
       " 'address king pop',\n",
       " 'address one',\n",
       " 'address one person',\n",
       " 'address section',\n",
       " 'address section 4',\n",
       " 'address someth',\n",
       " 'address someth els',\n",
       " 'adequ',\n",
       " 'adequ refer',\n",
       " 'adequ refer ad',\n",
       " 'adher',\n",
       " 'adher wikpedia',\n",
       " 'adher wikpedia standard',\n",
       " 'admin',\n",
       " 'admin desuckacrook',\n",
       " 'admin desuckacrook bulletand',\n",
       " 'admin even',\n",
       " 'admin even sadder',\n",
       " 'admin excus',\n",
       " 'admin excus ignor',\n",
       " 'admin get',\n",
       " 'admin get away',\n",
       " 'admin power',\n",
       " 'admin power stan',\n",
       " 'admin right',\n",
       " 'admin right see',\n",
       " 'administr',\n",
       " 'administr air',\n",
       " 'administr air date',\n",
       " 'administr base',\n",
       " 'administr base sign',\n",
       " 'administr call',\n",
       " 'administr call wikipedia',\n",
       " 'administr complaint',\n",
       " 'administr complaint file',\n",
       " 'administr continu',\n",
       " 'administr continu act',\n",
       " 'administr look',\n",
       " 'administr look histori',\n",
       " 'administr request',\n",
       " 'administr request help',\n",
       " 'administr tell',\n",
       " 'administr tell administr',\n",
       " 'administratotor',\n",
       " 'administratotor revers',\n",
       " 'administratotor revers thus',\n",
       " 'adminstr',\n",
       " 'adminstr came',\n",
       " 'adminstr came page',\n",
       " 'admir',\n",
       " 'admir disgrac',\n",
       " 'admir disgrac former',\n",
       " 'admit',\n",
       " 'admit made',\n",
       " 'admit made whole',\n",
       " 'adress',\n",
       " 'adult',\n",
       " 'adult admin',\n",
       " 'adult admin power',\n",
       " 'adult great',\n",
       " 'adult great section',\n",
       " 'adult time',\n",
       " 'adult time ridicul',\n",
       " 'advers',\n",
       " 'advers basic',\n",
       " 'advers basic inform',\n",
       " 'advert',\n",
       " 'advert anyth',\n",
       " 'advert anyth life',\n",
       " 'advert search',\n",
       " 'advert search engin',\n",
       " 'advis',\n",
       " 'advis u',\n",
       " 'advis u watch',\n",
       " 'aerahemnpov',\n",
       " 'aerahemnpov editor',\n",
       " 'afaik',\n",
       " 'afaik refer',\n",
       " 'afaik refer collision',\n",
       " 'affect',\n",
       " 'affect concoct',\n",
       " 'affect concoct offens',\n",
       " 'affili',\n",
       " 'affili inclus',\n",
       " 'affili inclus basic',\n",
       " 'afraid',\n",
       " 'afraid say',\n",
       " 'afraid say anyon',\n",
       " 'age',\n",
       " 'age modern',\n",
       " 'age modern human',\n",
       " 'agenda',\n",
       " 'agenda even',\n",
       " 'agenda even remov',\n",
       " 'ago',\n",
       " 'ago great',\n",
       " 'ago great power',\n",
       " 'ago poor',\n",
       " 'ago poor never',\n",
       " 'ago propos',\n",
       " 'ago propos section',\n",
       " 'ago think',\n",
       " 'ago think technic',\n",
       " 'ago wish',\n",
       " 'ago wish join',\n",
       " 'agre',\n",
       " 'agre delet',\n",
       " 'agre delet articl',\n",
       " 'agre enjoy',\n",
       " 'agre enjoy like',\n",
       " 'agre interpret',\n",
       " 'agre interpret denot',\n",
       " 'agre list',\n",
       " 'agre list garbag',\n",
       " 'agre refer',\n",
       " 'agre refer say',\n",
       " 'agre relev',\n",
       " 'agre relev inform',\n",
       " 'agre term',\n",
       " 'agre term seem',\n",
       " 'agre troll',\n",
       " 'agre troll snitch',\n",
       " 'agre wiki',\n",
       " 'agre wiki articl',\n",
       " 'agreement',\n",
       " 'agreement go',\n",
       " 'agreement go ahead',\n",
       " 'ahead',\n",
       " 'ahead add',\n",
       " 'ahead add paragraph',\n",
       " 'ahead censor',\n",
       " 'ahead censor interest',\n",
       " 'ahistor',\n",
       " 'ahistor vs',\n",
       " 'ahistor vs deriv',\n",
       " 'aid',\n",
       " 'aid denialist',\n",
       " 'aid denialist 13100',\n",
       " 'aid know',\n",
       " 'aid probabl',\n",
       " 'aid probabl bought',\n",
       " 'aid sad',\n",
       " 'aid sad mani',\n",
       " 'aim',\n",
       " 'aim articl',\n",
       " 'aim that',\n",
       " 'aim that problem',\n",
       " 'air',\n",
       " 'air date',\n",
       " 'air date minor',\n",
       " 'air date right',\n",
       " 'airlin',\n",
       " 'airlin one',\n",
       " 'airlin one disput',\n",
       " 'aka',\n",
       " 'aka 17419166126',\n",
       " 'aka 17419166126 aka',\n",
       " 'aka 1741916992',\n",
       " 'aka 1741916992 appar',\n",
       " 'aka 9925417230',\n",
       " 'aka 9925417230 remov',\n",
       " 'aka among',\n",
       " 'aka among other',\n",
       " 'akin',\n",
       " 'akin write',\n",
       " 'akin write mani',\n",
       " 'alan',\n",
       " 'alan whicker',\n",
       " 'alan whicker posit',\n",
       " 'album',\n",
       " 'album 1987',\n",
       " 'album 1987 sold',\n",
       " 'album 1989',\n",
       " 'album 1989 jackson',\n",
       " 'album danger',\n",
       " 'album danger thorough',\n",
       " 'album estim',\n",
       " 'album estim percentag',\n",
       " 'album review',\n",
       " 'album review wersorg',\n",
       " 'album werent',\n",
       " 'album werent good',\n",
       " 'album yeah',\n",
       " 'album yeah wj',\n",
       " 'alert',\n",
       " 'alert good',\n",
       " 'alert good call',\n",
       " 'allboy',\n",
       " 'allboy religi',\n",
       " 'allboy religi inform',\n",
       " 'alleg',\n",
       " 'alleg articl',\n",
       " 'alleg articl true',\n",
       " 'alleg obscur',\n",
       " 'alleg obscur antipsychiatri',\n",
       " 'alleg shortcom',\n",
       " 'alleg shortcom board',\n",
       " 'allgirl',\n",
       " 'allgirl religi',\n",
       " 'allgirl religi inform',\n",
       " 'allgirl school',\n",
       " 'allgirl school holi',\n",
       " 'allow',\n",
       " 'allow deleat',\n",
       " 'allow deleat imag',\n",
       " 'allow wikipedia',\n",
       " 'allow wikipedia use',\n",
       " 'almost',\n",
       " 'almost 100',\n",
       " 'almost 100 new',\n",
       " 'almost entir',\n",
       " 'almost entir articl',\n",
       " 'almost jackson',\n",
       " 'almost jackson remain',\n",
       " 'almost like',\n",
       " 'almost like liber',\n",
       " 'almost use',\n",
       " 'almost use readership',\n",
       " 'alon',\n",
       " 'alon alreadi',\n",
       " 'alon alreadi problem',\n",
       " 'alon racist',\n",
       " 'alon racist wacko',\n",
       " 'along',\n",
       " 'along bankruptci',\n",
       " 'along bankruptci even',\n",
       " 'alreadi',\n",
       " 'alreadi bad',\n",
       " 'alreadi bad quit',\n",
       " 'alreadi finish',\n",
       " 'alreadi finish main',\n",
       " 'alreadi problem',\n",
       " 'alreadi said',\n",
       " 'alreadi said page',\n",
       " 'alreadi send',\n",
       " 'alreadi send user',\n",
       " 'alreadi suspect',\n",
       " 'alreadi suspect may',\n",
       " 'alreadi took',\n",
       " 'alreadi took place',\n",
       " 'alright',\n",
       " 'alright lack',\n",
       " 'alright lack fact',\n",
       " 'also',\n",
       " 'also absolut',\n",
       " 'also absolut unequivoc',\n",
       " 'also base',\n",
       " 'also base 32',\n",
       " 'also beliv',\n",
       " 'also beliv edit',\n",
       " 'also data',\n",
       " 'also data well',\n",
       " 'also delet',\n",
       " 'also delet even',\n",
       " 'also forward',\n",
       " 'also forward administr',\n",
       " 'also found',\n",
       " 'also found use',\n",
       " 'also invit',\n",
       " 'also invit everybodi',\n",
       " 'also irrelav',\n",
       " 'also irrelav smcs',\n",
       " 'also kind',\n",
       " 'also kind indic',\n",
       " 'also man',\n",
       " 'also man least',\n",
       " 'also noth',\n",
       " 'also noth histori',\n",
       " 'also one',\n",
       " 'also one guid',\n",
       " 'also pleas',\n",
       " 'also pleas consid',\n",
       " 'also pleas read',\n",
       " 'also record',\n",
       " 'also record indic',\n",
       " 'also revert',\n",
       " 'also revert pure',\n",
       " 'also say',\n",
       " 'also say your',\n",
       " 'also unlock',\n",
       " 'also unlock chase',\n",
       " 'also wikipedia',\n",
       " 'also wikipedia wikipedia',\n",
       " 'also worthi',\n",
       " 'also worthi mention',\n",
       " 'also write',\n",
       " 'also write proper',\n",
       " 'alter',\n",
       " 'alter filehildebrandtgreg',\n",
       " 'alter filehildebrandtgreg timjpg',\n",
       " 'altogeth',\n",
       " 'altogeth least',\n",
       " 'altogeth least add',\n",
       " 'altruist',\n",
       " 'altruist word',\n",
       " 'altruist word action',\n",
       " 'alway',\n",
       " 'alway concis',\n",
       " 'alway concis relev',\n",
       " 'alway one',\n",
       " 'alway one click',\n",
       " 'alway print',\n",
       " 'alway print everi',\n",
       " 'alway solv',\n",
       " 'alway solv high',\n",
       " 'amazon',\n",
       " 'amazon addit',\n",
       " 'amazon addit taken',\n",
       " 'america',\n",
       " 'america award',\n",
       " 'american',\n",
       " 'american america',\n",
       " 'american appl',\n",
       " 'american appl pie',\n",
       " 'american eagl',\n",
       " 'american eagl outfitt',\n",
       " 'american right',\n",
       " 'american right degre',\n",
       " 'among',\n",
       " 'among other',\n",
       " 'among other cant',\n",
       " 'amount',\n",
       " 'amount rock',\n",
       " 'amount rock uplift',\n",
       " 'ancestor',\n",
       " 'ancestor centuri',\n",
       " 'ancestor centuri tri',\n",
       " 'ancestor worship',\n",
       " 'ancestor worship tradit',\n",
       " 'ancestri',\n",
       " 'ancestri subject',\n",
       " 'ancestri subject seen',\n",
       " 'andor',\n",
       " 'andor advert',\n",
       " 'andor advert search',\n",
       " 'angri',\n",
       " 'angri antisoci',\n",
       " 'angri antisoci person',\n",
       " 'angri grrrrrrrrrrrr',\n",
       " 'annon',\n",
       " 'annon uncivilis',\n",
       " 'annon uncivilis wiki',\n",
       " 'annoy',\n",
       " 'annoy editor',\n",
       " 'annoy favourit',\n",
       " 'annoy favourit past',\n",
       " 'annoy talk',\n",
       " 'annoy talk head',\n",
       " 'anon',\n",
       " 'anon ip',\n",
       " 'anon ip temp',\n",
       " 'anon rais',\n",
       " 'anon rais issu',\n",
       " 'anonymiss',\n",
       " 'anonymiss madchen',\n",
       " 'anonymiss madchen given',\n",
       " 'anoth',\n",
       " 'anoth abus',\n",
       " 'anoth abus admin',\n",
       " 'anoth ident',\n",
       " 'anoth ident anoth',\n",
       " 'anoth ip',\n",
       " 'anoth ip tri',\n",
       " 'answer',\n",
       " 'answer dont',\n",
       " 'answer dont tell',\n",
       " 'answer question',\n",
       " 'answer question hypocrit',\n",
       " 'answer question pleas',\n",
       " 'answer question welcom',\n",
       " 'answer simpl',\n",
       " 'answer simpl question',\n",
       " 'answer ulliloqu',\n",
       " 'answer ulliloqu much',\n",
       " 'antagon',\n",
       " 'antagon comment',\n",
       " 'antagon comment first',\n",
       " 'anti',\n",
       " 'anti neutral',\n",
       " 'anti neutral agenda',\n",
       " 'antiislam',\n",
       " 'antiislam cut',\n",
       " 'antiislam cut past',\n",
       " 'antiknowledg',\n",
       " 'antiknowledg hater',\n",
       " 'antipsychiatri',\n",
       " 'antipsychiatri book',\n",
       " 'antipsychiatri book fact',\n",
       " 'antisoci',\n",
       " 'antisoci person',\n",
       " 'antisoci person today',\n",
       " 'anybodi',\n",
       " 'anybodi ban',\n",
       " 'anybodi ban vandal',\n",
       " 'anybodi read',\n",
       " 'anybodi read insecur',\n",
       " 'anybodi wannab',\n",
       " 'anybodi wannab admin',\n",
       " 'anyhow',\n",
       " 'anyhow busi',\n",
       " 'anyhow busi peopl',\n",
       " 'anymor',\n",
       " 'anymor real',\n",
       " 'anymor real reason',\n",
       " 'anyon',\n",
       " 'anyon agre',\n",
       " 'anyon agre interpret',\n",
       " 'anyon els',\n",
       " 'anyon els agre',\n",
       " 'anyon see',\n",
       " 'anyth',\n",
       " 'anyth accur',\n",
       " 'anyth accur pepl',\n",
       " 'anyth cant',\n",
       " 'anyth cant believ',\n",
       " 'anyth day',\n",
       " 'anyth day get',\n",
       " 'anyth life',\n",
       " 'anyth life real',\n",
       " 'anyth like',\n",
       " 'anyth like care',\n",
       " 'anyth like didnt',\n",
       " 'anyth meant',\n",
       " 'anyth meant call',\n",
       " 'anyth non',\n",
       " 'anyth non bind',\n",
       " 'anyway',\n",
       " 'anyway peopl',\n",
       " 'anyway peopl know',\n",
       " 'apart',\n",
       " 'apart stop',\n",
       " 'apart stop hide',\n",
       " 'appar',\n",
       " 'appar edit',\n",
       " 'appar edit ted',\n",
       " 'appar made',\n",
       " 'appar made scientif',\n",
       " 'appar make',\n",
       " 'appar make sure',\n",
       " 'appeal',\n",
       " 'appeal made',\n",
       " 'appeal made bot',\n",
       " 'appear',\n",
       " 'appear constitut',\n",
       " 'appear constitut vandal',\n",
       " 'appear mere',\n",
       " 'appear mere ip',\n",
       " 'appear person',\n",
       " 'appear person group',\n",
       " 'appear selfserv',\n",
       " 'appear selfserv pr',\n",
       " 'appear tri',\n",
       " 'appear tri present',\n",
       " 'appear uncontruct',\n",
       " 'appear uncontruct sinc',\n",
       " 'appl',\n",
       " 'appl pie',\n",
       " 'appl pie date',\n",
       " 'appl pie imag',\n",
       " 'appli',\n",
       " 'appli delet',\n",
       " 'appli delet review',\n",
       " 'appoint',\n",
       " 'appoint psychiatristor',\n",
       " 'appoint psychiatristor gynecologist',\n",
       " 'appreci',\n",
       " 'appreci articl',\n",
       " 'appreci articl may',\n",
       " 'appreci everyon',\n",
       " 'appreci everyon includ',\n",
       " 'approach',\n",
       " 'appropri',\n",
       " 'appropri contain',\n",
       " 'appropri contain basic',\n",
       " 'appropri sourc',\n",
       " 'appropri sourc citat',\n",
       " 'appropri tag',\n",
       " 'appropri tag user',\n",
       " 'approv',\n",
       " 'approv foundat',\n",
       " 'april',\n",
       " 'april 2009',\n",
       " 'april 2009 pleas',\n",
       " 'architectur',\n",
       " 'architectur renam',\n",
       " 'architectur renam cultur',\n",
       " 'archiv',\n",
       " 'archiv realli',\n",
       " 'archiv realli want',\n",
       " 'area',\n",
       " 'area foldbelt',\n",
       " 'area foldbelt isnt',\n",
       " 'area ragib',\n",
       " 'area ragib obvious',\n",
       " 'area second',\n",
       " 'area second question',\n",
       " 'area seem',\n",
       " 'area seem blind',\n",
       " 'arent',\n",
       " 'arent even',\n",
       " 'arent even product',\n",
       " 'arent part',\n",
       " 'arent part gaytourag',\n",
       " 'arer',\n",
       " 'arer zapatanca',\n",
       " 'arer zapatanca may',\n",
       " 'argentina',\n",
       " 'argentina articl',\n",
       " 'argentina articl congratul',\n",
       " 'argu',\n",
       " 'argu diplomat',\n",
       " 'argu diplomat consult',\n",
       " 'argument',\n",
       " 'argument alan',\n",
       " 'argument alan whicker',\n",
       " 'arizona',\n",
       " 'arizona aka',\n",
       " 'arizona aka 17419166126',\n",
       " 'arm',\n",
       " 'arm song',\n",
       " 'arm song possess',\n",
       " 'around',\n",
       " 'around dream',\n",
       " 'around dream think',\n",
       " 'around especi',\n",
       " 'around especi littl',\n",
       " 'around sourc',\n",
       " 'around sourc persian',\n",
       " 'around temporari',\n",
       " 'around temporari block',\n",
       " 'arrhgh',\n",
       " 'arrhgh frederica',\n",
       " 'arrhgh frederica annoy',\n",
       " 'arrog',\n",
       " 'arrog entir',\n",
       " 'arrog entir inappropri',\n",
       " 'arrog guy',\n",
       " 'arrog person',\n",
       " 'arrog person think',\n",
       " 'arsewhol',\n",
       " 'arsewhol autom',\n",
       " 'arsewhol autom filter',\n",
       " 'articl',\n",
       " 'articl 1996',\n",
       " 'articl 1996 usual',\n",
       " 'articl accept',\n",
       " 'articl accept jack',\n",
       " 'articl add',\n",
       " 'articl add effect',\n",
       " 'articl address',\n",
       " 'articl address issu',\n",
       " 'articl also',\n",
       " 'articl also one',\n",
       " 'articl also say',\n",
       " 'articl anonymiss',\n",
       " 'articl anonymiss madchen',\n",
       " 'articl appear',\n",
       " 'articl appear person',\n",
       " 'articl appreci',\n",
       " 'articl appreci everyon',\n",
       " 'articl assert',\n",
       " 'articl assert subject',\n",
       " 'articl besid',\n",
       " 'articl besid last',\n",
       " 'articl bit',\n",
       " 'articl bit avoid',\n",
       " 'articl blog',\n",
       " 'articl blog true',\n",
       " 'articl call',\n",
       " 'articl call sultan',\n",
       " 'articl call us',\n",
       " 'articl cant',\n",
       " 'articl cant anyth',\n",
       " 'articl cant link',\n",
       " 'articl carl',\n",
       " 'articl carl grissom',\n",
       " 'articl conclus',\n",
       " 'articl conclus answer',\n",
       " 'articl confirm',\n",
       " 'articl confirm subject',\n",
       " 'articl congratul',\n",
       " 'articl copi',\n",
       " 'articl copi homepag',\n",
       " 'articl delet',\n",
       " 'articl delet may',\n",
       " 'articl devot',\n",
       " 'articl devot point',\n",
       " 'articl diatrib',\n",
       " 'articl diatrib denounc',\n",
       " 'articl discuss',\n",
       " 'articl dr',\n",
       " 'articl dr manfr',\n",
       " 'articl edit',\n",
       " 'articl edit like',\n",
       " 'articl editor',\n",
       " 'articl editor assist',\n",
       " 'articl especi',\n",
       " 'articl especi bit',\n",
       " 'articl even',\n",
       " 'articl even ask',\n",
       " 'articl even though',\n",
       " 'articl fan',\n",
       " 'articl fan flavor',\n",
       " 'articl felician',\n",
       " 'articl felician sister',\n",
       " 'articl gd',\n",
       " 'articl hi',\n",
       " 'articl hi kansa',\n",
       " 'articl im',\n",
       " 'articl im go',\n",
       " 'articl improv',\n",
       " ...]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_to_list(matrix):\n",
    "    matrix = matrix.toarray().round(decimals=3)\n",
    "    return matrix.tolist()\n",
    "score_list = np.array(matrix_to_list(tfidf_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array(df['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(score_list, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196, 10000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 10000)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red\">Using tensorflow on a sparse dataset is kinda useless but whatever </h2>\n",
    "\n",
    "### Code copied shamelessly from [Josh Meyer's Website](http://jrmeyer.github.io/machinelearning/2019/05/29/tensorflow-dataset-estimator-api.html)\n",
    "\n",
    "## Creating a TF compatible dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfrecord(name, Xs, Ys):\n",
    "    with tf.compat.v1.python_io.TFRecordWriter('%s.tfrecords'%name) as writer:\n",
    "        for i in range(len(Xs)):\n",
    "            label = int(Ys[i])\n",
    "            feats = np.array([ float(feat) for feat in Xs[i] ]).tostring()\n",
    "            example = tf.train.Example()\n",
    "            example.features.feature[\"feats\"].bytes_list.value.append(feats)\n",
    "            example.features.feature[\"label\"].int64_list.value.append(label)\n",
    "            writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecord('train', X_train, y_train)\n",
    "tfrecord('eval', X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function to parse data types for the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(record):\n",
    "    '''\n",
    "    This is a parser function. It defines the template for\n",
    "    interpreting the examples you're feeding in. Basically, \n",
    "    this function defines what the labels and data look like\n",
    "    for your labeled data. \n",
    "    '''\n",
    "\n",
    "    # the 'features' here include your normal data feats along\n",
    "    # with the label for that data\n",
    "    features={\n",
    "      'feats': tf.compat.v1.FixedLenFeature([], tf.string),\n",
    "      'label': tf.compat.v1.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "\n",
    "    parsed = tf.compat.v1.parse_single_example(record, features)\n",
    "\n",
    "    # some conversion and casting to get from bytes to floats and ints\n",
    "    feats= tf.convert_to_tensor(tf.compat.v1.decode_raw(parsed['feats'], tf.float64))\n",
    "    label= tf.cast(parsed['label'], tf.int64)\n",
    "\n",
    "    # since you can have multiple kinds of feats, you return a dictionary for feats\n",
    "    # but only an int for the label\n",
    "    return {'feats': feats}, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function for interative evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_input_fn(tfrecords_path):\n",
    "    dataset = (\n",
    "    tf.data.TFRecordDataset(tfrecords_path)\n",
    "    .map(parser)\n",
    "    .batch(16)\n",
    "    )\n",
    "\n",
    "    iterator = tf.compat.v1.data.make_one_shot_iterator(dataset)\n",
    "\n",
    "    batch_feats, batch_labels = iterator.get_next()\n",
    "\n",
    "    return batch_feats, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x130855b50>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "DNNClassifier = tf.estimator.DNNClassifier(\n",
    "  feature_columns = [tf.feature_column.numeric_column(key='feats', dtype=tf.float64, shape=(10000,))],\n",
    "  hidden_units = [128, 128, 128, 128],\n",
    "  n_classes = 2,\n",
    "  model_dir = '/tmp/tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/head/binary_class_head.py:206: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/adagrad.py:108: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf/model.ckpt-26\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file utilities to get mtimes.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 26 into /tmp/tf/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.69215167, step = 26\n",
      "INFO:tensorflow:Saving checkpoints for 39 into /tmp/tf/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-10-03T19:36:46Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf/model.ckpt-39\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-10-03-19:36:47\n",
      "INFO:tensorflow:Saving dict for global step 39: accuracy = 0.6, accuracy_baseline = 0.6, auc = 0.49166667, auc_precision_recall = 0.59588945, average_loss = 0.6917891, global_step = 39, label/mean = 0.6, loss = 0.6920581, precision = 0.6, prediction/mean = 0.5035713, recall = 1.0\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 39: /tmp/tf/model.ckpt-39\n",
      "INFO:tensorflow:Loss for final step: 0.69760895.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'accuracy': 0.6,\n",
       "  'accuracy_baseline': 0.6,\n",
       "  'auc': 0.49166667,\n",
       "  'auc_precision_recall': 0.59588945,\n",
       "  'average_loss': 0.6917891,\n",
       "  'label/mean': 0.6,\n",
       "  'loss': 0.6920581,\n",
       "  'precision': 0.6,\n",
       "  'prediction/mean': 0.5035713,\n",
       "  'recall': 1.0,\n",
       "  'global_step': 39},\n",
       " [])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_spec_dnn = tf.estimator.TrainSpec(input_fn = lambda: my_input_fn('train.tfrecords') , max_steps=10000)\n",
    "eval_spec_dnn = tf.estimator.EvalSpec(input_fn = lambda: my_input_fn('eval.tfrecords') )\n",
    "\n",
    "tf.estimator.train_and_evaluate(DNNClassifier, train_spec_dnn, eval_spec_dnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out sklearn instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Model(LBFGS) Accuracy = 60.00%\n",
      "CPU times: user 13.8 s, sys: 2.18 s, total: 16 s\n",
      "Wall time: 7.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_mlp = MLPClassifier(solver='lbfgs', alpha=1e-2, \n",
    "                        hidden_layer_sizes=(256,256,2,256), \n",
    "                        max_iter=10000, random_state=123)\n",
    "clf_mlp.fit(X=X_train, y = y_train)\n",
    "y_pred = clf_mlp.predict(X_test)\n",
    "print(\"Neural Model(LBFGS) Accuracy = %.2f%%\"%(np.mean(y_pred==y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy = 62.00%\n",
      "CPU times: user 342 ms, sys: 16.3 ms, total: 358 ms\n",
      "Wall time: 364 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_rf = RandomForestClassifier(random_state=12, max_depth=10, n_estimators=100)\n",
    "clf_rf.fit(X = X_train, y = y_train)\n",
    "y_pred = clf_rf.predict(X_test)\n",
    "print(\"Random Forest Accuracy = %.2f%%\"%(np.mean(y_pred==y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.251, 0.0, 0.0, 0.0, 0.177, 0.177, 0.177, 0.177, 0.177, 0.177, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.353, 0.353, 0.177, 0.177, 0.177, 0.177, 0.177, 0.177, 0.177, 0.177, 0.177, 0.353, 0.177, 0.177, 0.177], [0.216, 0.216, 0.216, 0.216, 0.216, 0.216, 0.153, 0.216, 0.216, 0.216, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.216, 0.216, 0.216, 0.216, 0.216, 0.216, 0.216, 0.216, 0.216, 0.216, 0.216, 0.216, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/swarupsahoo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "stemmer = SnowballStemmer('english')\n",
    "p_map = dict((ord(char),None) for char in string.punctuation)\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(item) for item in tokens]\n",
    "def normalize(text):\n",
    "    return stem_tokens(nltk.word_tokenize(text.lower().translate(p_map)))\n",
    "\n",
    "\n",
    "a = \"oh wow dell sucks so much, dell is the worst oh wow\"\n",
    "b = \"dell laptops are so good, much better than those dirt cheap macbooks\"\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english', \n",
    "                        use_idf=True, ngram_range=(1,4), \n",
    "                        strip_accents='unicode', \n",
    "                        tokenizer=normalize)\n",
    "\n",
    "tfidf_score = tfidf.fit_transform([a, b])\n",
    "\n",
    "def matrix_to_list(matrix):\n",
    "    matrix = matrix.toarray().round(decimals=3)\n",
    "    return matrix.tolist()\n",
    "score_list = matrix_to_list(tfidf_score)\n",
    "\n",
    "print(score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['better',\n",
       " 'better dirt',\n",
       " 'better dirt cheap',\n",
       " 'better dirt cheap macbook',\n",
       " 'cheap',\n",
       " 'cheap macbook',\n",
       " 'dell',\n",
       " 'dell laptop',\n",
       " 'dell laptop good',\n",
       " 'dell laptop good better',\n",
       " 'dell suck',\n",
       " 'dell suck dell',\n",
       " 'dell suck dell worst',\n",
       " 'dell worst',\n",
       " 'dell worst oh',\n",
       " 'dell worst oh wow',\n",
       " 'dirt',\n",
       " 'dirt cheap',\n",
       " 'dirt cheap macbook',\n",
       " 'good',\n",
       " 'good better',\n",
       " 'good better dirt',\n",
       " 'good better dirt cheap',\n",
       " 'laptop',\n",
       " 'laptop good',\n",
       " 'laptop good better',\n",
       " 'laptop good better dirt',\n",
       " 'macbook',\n",
       " 'oh',\n",
       " 'oh wow',\n",
       " 'oh wow dell',\n",
       " 'oh wow dell suck',\n",
       " 'suck',\n",
       " 'suck dell',\n",
       " 'suck dell worst',\n",
       " 'suck dell worst oh',\n",
       " 'worst',\n",
       " 'worst oh',\n",
       " 'worst oh wow',\n",
       " 'wow',\n",
       " 'wow dell',\n",
       " 'wow dell suck',\n",
       " 'wow dell suck dell']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
